set -x

NOW=$(date +"%Y%m%d_%H_%M")
EXPERIMENT_NAME="qwen7b_sft2_${NOW}"
LOG_DIR="logs"
mkdir -p ${LOG_DIR}

# Run the PPO training with complete configuration
python3 -m verl.trainer.main_ppo \
  data.train_files=/root/data/gsm8k/train.parquet \
  data.val_files=/root/data/gsm8k/test.parquet \
  data.prompt_key=prompt \
  data.max_prompt_length=512 \
  data.max_response_length=512 \
  data.train_batch_size=8 \
  data.val_batch_size=8 \
  data.return_raw_input_ids=True \
  data.return_raw_chat=True \
  data.shuffle=False \
  data.filter_overlong_prompts=False \
  actor_rollout_ref.hybrid_engine=True \
  actor_rollout_ref.model.path=Qwen/Qwen2.5-7B \
  actor_rollout_ref.model.external_lib=null \
  actor_rollout_ref.model.override_config={} \
  actor_rollout_ref.model.enable_gradient_checkpointing=True \
  actor_rollout_ref.model.use_remove_padding=True \
  +actor_rollout_ref.model.trust_remote_code=True \
  actor_rollout_ref.actor.strategy=fsdp \
  actor_rollout_ref.actor.ppo_mini_batch_size=4 \
  actor_rollout_ref.actor.ppo_micro_batch_size=null \
  +actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=1 \
  actor_rollout_ref.actor.use_dynamic_bsz=True \
  actor_rollout_ref.actor.ppo_max_token_len_per_gpu=32768 \
  actor_rollout_ref.actor.grad_clip=0.5 \
  actor_rollout_ref.actor.clip_ratio=0.2 \
  actor_rollout_ref.actor.clip_ratio_c=3.0 \
  +actor_rollout_ref.actor.loss_agg_mode="token-mean" \
  actor_rollout_ref.actor.entropy_coeff=0.0 \
  actor_rollout_ref.actor.use_kl_loss=True \
  actor_rollout_ref.actor.kl_loss_coef=0.0001 \
  actor_rollout_ref.actor.kl_loss_type=low_var_kl \
  actor_rollout_ref.actor.ppo_epochs=1 \
  actor_rollout_ref.actor.shuffle=False \
  actor_rollout_ref.actor.ulysses_sequence_parallel_size=1 \
  actor_rollout_ref.actor.checkpoint.contents=['model','optimizer','extra'] \
  actor_rollout_ref.actor.optim.lr=1e-6 \
  actor_rollout_ref.actor.fsdp_config.wrap_policy.min_num_params=0 \
  actor_rollout_ref.actor.fsdp_config.param_offload=True \
  actor_rollout_ref.actor.fsdp_config.optimizer_offload=True \
  actor_rollout_ref.actor.fsdp_config.fsdp_size=-1 \
  actor_rollout_ref.ref.fsdp_config.param_offload=True \
  actor_rollout_ref.ref.fsdp_config.wrap_policy.min_num_params=0 \
  actor_rollout_ref.ref.log_prob_micro_batch_size=null \
  actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=null \
  actor_rollout_ref.ref.log_prob_use_dynamic_bsz=\${actor_rollout_ref.actor.use_dynamic_bsz} \
  actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=\${actor_rollout_ref.actor.ppo_max_token_len_per_gpu} \
  actor_rollout_ref.ref.ulysses_sequence_parallel_size=\${actor_rollout_ref.actor.ulysses_sequence_parallel_size} \
  actor_rollout_ref.rollout.name=sglang_async \
  actor_rollout_ref.rollout.prompt_length=\${data.max_prompt_length} \
  actor_rollout_ref.rollout.response_length=\${data.max_response_length} \
  actor_rollout_ref.rollout.max_model_len=null \
  actor_rollout_ref.rollout.dtype=bfloat16 \
  actor_rollout_ref.rollout.temperature=\${actor_rollout_ref.rollout.sampling_params.temperature} \
  actor_rollout_ref.rollout.gpu_memory_utilization=0.8 \
  +actor_rollout_ref.rollout.enable_memory_saver=False \
  actor_rollout_ref.rollout.ignore_eos=False \
  actor_rollout_ref.rollout.enforce_eager=True \
  actor_rollout_ref.rollout.free_cache_engine=True \
  actor_rollout_ref.rollout.load_format=dummy_dtensor \
  actor_rollout_ref.rollout.tensor_model_parallel_size=4 \
  actor_rollout_ref.rollout.log_prob_micro_batch_size=null \
  actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=null \
  actor_rollout_ref.rollout.log_prob_use_dynamic_bsz=\${actor_rollout_ref.actor.use_dynamic_bsz} \
  actor_rollout_ref.rollout.log_prob_max_token_len_per_gpu=\${actor_rollout_ref.actor.ppo_max_token_len_per_gpu} \
  actor_rollout_ref.rollout.disable_log_stats=True \
  actor_rollout_ref.rollout.enable_chunked_prefill=True \
  actor_rollout_ref.rollout.do_sample=True \
  actor_rollout_ref.rollout.n=1 \
  +actor_rollout_ref.rollout.max_turns=3 \
  +actor_rollout_ref.rollout.plugin_browser=False \
  +actor_rollout_ref.rollout.path=\${actor_rollout_ref.model.path} \
  +actor_rollout_ref.rollout.sampling_params.temperature=0.8 \
  +actor_rollout_ref.rollout.sampling_params.max_new_tokens=192 \
  +actor_rollout_ref.rollout.sampling_params.stop=[] \
  actor_rollout_ref.rollout.val_kwargs.top_k=-1 \
  actor_rollout_ref.rollout.val_kwargs.top_p=1.0 \
  actor_rollout_ref.rollout.val_kwargs.temperature=0 \
  actor_rollout_ref.rollout.val_kwargs.n=1 \
  actor_rollout_ref.rollout.val_kwargs.do_sample=False \
  actor_rollout_ref.rollout.tool_kwargs.tools_config_file="examples/sglang_multiturn/config/tool_config/gsm8k_tool_config.yaml" \
  critic.strategy=fsdp \
  critic.optim.lr=1e-5 \
  critic.model.path=Qwen/Qwen2.5-7B \
  critic.model.tokenizer_path=\${actor_rollout_ref.model.path} \
  critic.model.override_config={} \
  critic.model.external_lib=\${actor_rollout_ref.model.external_lib} \
  critic.model.enable_gradient_checkpointing=True \
  critic.model.use_remove_padding=False \
  critic.model.fsdp_config.param_offload=False \
  critic.model.fsdp_config.optimizer_offload=False \
  critic.model.fsdp_config.wrap_policy.min_num_params=0 \
  critic.model.fsdp_config.fsdp_size=-1 \
  critic.ppo_mini_batch_size=\${actor_rollout_ref.actor.ppo_mini_batch_size} \
  critic.ppo_micro_batch_size=null \
  critic.ppo_micro_batch_size_per_gpu=null \
  critic.forward_micro_batch_size=\${critic.ppo_micro_batch_size} \
  critic.forward_micro_batch_size_per_gpu=\${critic.ppo_micro_batch_size_per_gpu} \
  critic.use_dynamic_bsz=\${actor_rollout_ref.actor.use_dynamic_bsz} \
  critic.ppo_max_token_len_per_gpu=32768 \
  critic.forward_max_token_len_per_gpu=\${critic.ppo_max_token_len_per_gpu} \
  critic.ulysses_sequence_parallel_size=1 \
  critic.ppo_epochs=\${actor_rollout_ref.actor.ppo_epochs} \
  critic.shuffle=\${actor_rollout_ref.actor.shuffle} \
  critic.grad_clip=1.0 \
  critic.cliprange_value=0.5 \
  critic.checkpoint.contents=['model','optimizer','extra'] \
  reward_model.enable=False \
  algorithm.gamma=1.0 \
  algorithm.lam=1.0 \
  algorithm.adv_estimator=grpo \
  algorithm.use_kl_in_reward=False \
  algorithm.kl_penalty=kl \
  algorithm.kl_ctrl.type=fixed \
  algorithm.kl_ctrl.kl_coef=0.001 \
  +trainer.hybrid_engine=True \
  trainer.total_epochs=3 \
  trainer.total_training_steps=null \
  trainer.project_name=gsm8k_async_rl \
  trainer.experiment_name="${EXPERIMENT_NAME}" \
  trainer.logger=['console','wandb'] \
  +trainer.val_generations_to_log_to_wandb=0 \
  trainer.nnodes=1 \
  trainer.n_gpus_per_node=4 \
  trainer.resume_mode=auto \
  +trainer.resume_from_path=False \
  trainer.test_freq=-1 \
  trainer.critic_warmup=0 \
  trainer.default_hdfs_dir=null \
  +trainer.remove_previous_ckpt_in_save=True \
  trainer.del_local_ckpt_after_load=True \
  trainer.default_local_dir=/workspace/gsm8k/ckpt/\${trainer.project_name}/\${trainer.experiment_name} \
  trainer.val_before_train=False \
  trainer.balance_batch=False \
  | tee ${LOG_DIR}/${EXPERIMENT_NAME}.log
